{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"capsule.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"2eWlqu5oOGR5","colab_type":"code","outputId":"41d5f26e-67a7-4ebe-d503-c9328bd71d97","executionInfo":{"status":"ok","timestamp":1555329210767,"user_tz":-330,"elapsed":1605,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"metadata":{"id":"YOwUmTTgOKK-","colab_type":"code","outputId":"c839e94f-8c22-44a0-ba8a-67c8b4625618","executionInfo":{"status":"ok","timestamp":1555329214867,"user_tz":-330,"elapsed":1322,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["cd /content/drive/My Drive/Emotion Recogition/code/python_files"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Emotion Recogition/code/python_files\n"],"name":"stdout"}]},{"metadata":{"id":"pO4SIxQTOKIF","colab_type":"code","outputId":"69b7e023-0859-4358-c0cc-8f8ac1fa1802","executionInfo":{"status":"ok","timestamp":1555329221612,"user_tz":-330,"elapsed":2933,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["import numpy as np\n","import os\n","import sys\n","import pandas as pd\n","\n","import wave\n","from keras.callbacks import EarlyStopping\n","from sklearn.utils import class_weight\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","import keras\n","from keras.models import Sequential, Model\n","from sklearn.preprocessing import scale\n","from keras.layers.core import Dense, Activation\n","from keras.layers import LSTM, Input, Flatten,Dropout,GlobalAveragePooling2D,MaxPooling2D\n","from keras.layers.convolutional import Conv2D\n","from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing import sequence\n","from scipy import signal\n","import matplotlib.pyplot as plot\n","from helper import *\n","from sklearn.preprocessing import LabelEncoder\n","from Layers import *\n","from Capsnet import CapsNet \n","from keras import layers,losses,models,optimizers\n","from keras import backend as K\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"6Ju8pMtS15__","colab_type":"code","colab":{}},"cell_type":"code","source":["from helper import *"],"execution_count":0,"outputs":[]},{"metadata":{"id":"koWvKufzOKE_","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","code_path = os.path.dirname(os.path.realpath(os.getcwd()))\n","emotions_used = np.array(['ang', 'exc', 'neu', 'sad'])\n","data_path = code_path + \"/../data/sessions/\"\n","sessions = ['Session1', 'Session2', 'Session3', 'Session4', 'Session5']\n","framerate = 16000"],"execution_count":0,"outputs":[]},{"metadata":{"id":"G7Nr_oXDOKDa","colab_type":"code","colab":{}},"cell_type":"code","source":["import pickle\n","with open(code_path + '/../data/'+'data_collected.pickle', 'rb') as handle:\n","    data2 = pickle.load(handle)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"H3s-xLiJKc-k","colab_type":"code","outputId":"7b732423-968e-4ab7-d74b-58613dd55f85","executionInfo":{"status":"ok","timestamp":1555329242564,"user_tz":-330,"elapsed":4621,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["!pip install python_speech_features  "],"execution_count":7,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: python_speech_features in /usr/local/lib/python3.6/dist-packages (0.6)\n"],"name":"stdout"}]},{"metadata":{"id":"QcMRPjXxKZiX","colab_type":"code","colab":{}},"cell_type":"code","source":["from python_speech_features import mfcc\n","from python_speech_features import logfbank\n","import scipy.io.wavfile as wav"],"execution_count":0,"outputs":[]},{"metadata":{"id":"odDNiRMwOKAD","colab_type":"code","outputId":"648a6371-d5bb-4356-eb61-4030d88ab011","executionInfo":{"status":"ok","timestamp":1555329311000,"user_tz":-330,"elapsed":47732,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"cell_type":"code","source":["\n","X_train = []\n","X_test = []\n","Y_train = []\n","Y_test = []\n","fs = 16e3\n","counter = 0\n","counter1 = 0\n","for ses_mod in data2:\n","    if 'impro' in ses_mod['id'] and ses_mod['emotion'] in emotions_used:\n","        fbank_feat = logfbank(ses_mod['signal'],nfilt=40)\n","        Sxx = np.transpose(fbank_feat)\n","        Sxx = pad_sequence_into_array(Sxx,maxlen=300,value=0)\n","        Sxx = scale(Sxx)\n","        \n","        if ses_mod['id'][:5]==\"Ses05\":\n","            counter+=1\n","            X_test.append(Sxx)\n","            Y_test.append(ses_mod['emotion'])\n","        else:\n","            counter1+=1\n","            X_train.append(Sxx)\n","            Y_train.append(ses_mod['emotion'])\n","        \n","print(counter)\n","print(counter1)\n","\n","X_test = np.array(X_test)\n","X_train = np.array(X_train)\n","print(X_train.shape)\n","print(X_test.shape)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["625\n","2034\n","(2034, 40, 300)\n","(625, 40, 300)\n"],"name":"stdout"}]},{"metadata":{"id":"Mnd_pyQxT8py","colab_type":"code","colab":{}},"cell_type":"code","source":["y  = pd.get_dummies(Y_train+Y_test)\n","y_train = y[0:len(Y_train)]\n","y_test = y[len(Y_train):]\n","y_train = np.array(y_train)\n","y_test = np.array(y_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e9xkh_B_T82m","colab_type":"code","colab":{}},"cell_type":"code","source":["X_train = X_train.reshape(-1,40,300,1)\n","X_test = X_test.reshape(-1,40,300,1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0lOts3PXJGNJ","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","from keras import layers, models, optimizers\n","from keras import backend as K\n","import matplotlib.pyplot as plt\n","from capsulelayers import CapsuleLayer, PrimaryCap, Length, Mask"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dp4kwQJ3Lb9l","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","def CapsNet(input_shape, n_class, routings):\n","    \"\"\"\n","    A Capsule Network on MNIST.\n","    :param input_shape: data shape, 3d, [width, height, channels]\n","    :param n_class: number of classes\n","    :param routings: number of routing iterations\n","    :return: Two Keras Models, the first one used for training, and the second one for evaluation.\n","            `eval_model` can also be used for training.\n","    \"\"\"\n","    x = layers.Input(shape=input_shape)\n","\n","    # Layer 1: Just a conventional Conv2D layer\n","    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n","\n","    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_capsule]\n","    primarycaps = PrimaryCap(conv1, dim_capsule=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n","\n","    # Layer 3: Capsule layer. Routing algorithm works here.\n","    digitcaps = CapsuleLayer(num_capsule=n_class, dim_capsule=16, routings=routings,\n","                             name='digitcaps')(primarycaps)\n","\n","    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n","    # If using tensorflow, this will not be necessary. :)\n","    out_caps = Length(name='capsnet')(digitcaps)\n","\n","    # Decoder network.\n","    y = layers.Input(shape=(n_class,))\n","    masked_by_y = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer. For training\n","    masked = Mask()(digitcaps)  # Mask using the capsule with maximal length. For prediction\n","\n","    # Shared Decoder model in training and prediction\n","    decoder = models.Sequential(name='decoder')\n","    decoder.add(layers.Dense(512, activation='relu', input_dim=16*n_class))\n","    decoder.add(layers.Dense(1024, activation='relu'))\n","    decoder.add(layers.Dense(np.prod(input_shape), activation='sigmoid'))\n","    decoder.add(layers.Reshape(target_shape=input_shape, name='out_recon'))\n","\n","    # Models for training and evaluation (prediction)\n","    train_model = models.Model([x, y], [out_caps, decoder(masked_by_y)])\n","    eval_model = models.Model(x, [out_caps, decoder(masked)])\n","\n","    # manipulate model\n","    noise = layers.Input(shape=(n_class, 16))\n","    noised_digitcaps = layers.Add()([digitcaps, noise])\n","    masked_noised_y = Mask()([noised_digitcaps, y])\n","    manipulate_model = models.Model([x, y, noise], decoder(masked_noised_y))\n","    return train_model, eval_model, manipulate_model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kzfIhrirLw91","colab_type":"code","colab":{}},"cell_type":"code","source":["def margin_loss(y_true, y_pred):\n","    \"\"\"\n","    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n","    :param y_true: [None, n_classes]\n","    :param y_pred: [None, num_capsule]\n","    :return: a scalar loss value.\n","    \"\"\"\n","    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n","        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n","\n","    return K.mean(K.sum(L, 1))"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"code","id":"GBJQAGIFtfq8","colab":{}},"cell_type":"code","source":[" model, eval_model, manipulate_model = CapsNet(input_shape=X_train.shape[1:],\n","                                                  n_class=4,\n","                                                  routings=3)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QSAvXb8oOBv_","colab_type":"code","colab":{}},"cell_type":"code","source":[" # compile the model\n","    model.compile(optimizer=optimizers.Adam(lr=0.001,),\n","                  loss=[margin_loss, 'mse'],\n","                  loss_weights=[1., 0.392],\n","                  metrics={'capsnet': 'accuracy'})\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"JyhrCQ_ajqMF","colab_type":"code","outputId":"b7b5d972-2979-49a4-b5af-a437974e1c82","executionInfo":{"status":"ok","timestamp":1555329400453,"user_tz":-330,"elapsed":980,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":503}},"cell_type":"code","source":["model.summary()"],"execution_count":19,"outputs":[{"output_type":"stream","text":["__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            (None, 40, 300, 1)   0                                            \n","__________________________________________________________________________________________________\n","conv1 (Conv2D)                  (None, 32, 292, 256) 20992       input_4[0][0]                    \n","__________________________________________________________________________________________________\n","primarycap_conv2d (Conv2D)      (None, 12, 142, 256) 5308672     conv1[0][0]                      \n","__________________________________________________________________________________________________\n","primarycap_reshape (Reshape)    (None, 54528, 8)     0           primarycap_conv2d[0][0]          \n","__________________________________________________________________________________________________\n","primarycap_squash (Lambda)      (None, 54528, 8)     0           primarycap_reshape[0][0]         \n","__________________________________________________________________________________________________\n","digitcaps (CapsuleLayer)        (None, 4, 16)        27918336    primarycap_squash[0][0]          \n","__________________________________________________________________________________________________\n","input_5 (InputLayer)            (None, 4)            0                                            \n","__________________________________________________________________________________________________\n","mask_4 (Mask)                   (None, 64)           0           digitcaps[0][0]                  \n","                                                                 input_5[0][0]                    \n","__________________________________________________________________________________________________\n","capsnet (Length)                (None, 4)            0           digitcaps[0][0]                  \n","__________________________________________________________________________________________________\n","decoder (Sequential)            (None, 40, 300, 1)   12858592    mask_4[0][0]                     \n","==================================================================================================\n","Total params: 46,106,592\n","Trainable params: 46,106,592\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"jX4-MnWGPhaF","colab_type":"code","colab":{}},"cell_type":"code","source":[" # callbacks \n","   m_check = keras.callbacks.ModelCheckpoint(filepath = './capsulelog.h5', monitor='val_acc', save_best_only=True, mode='max', verbose=1 )\n","   #lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * (0.9 ** 50))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yAYy7zEalPOB","colab_type":"code","colab":{}},"cell_type":"code","source":["Y_label = ( [np.where(r==1)[0][0] for r in y_train] )\n","early_stopping_monitor = EarlyStopping( monitor='val_loss',patience=20,verbose=1)\n","m_check = keras.callbacks.ModelCheckpoint(filepath = './capsule_encoder.h5', monitor='val_loss', save_best_only=True, mode='min', verbose=1 )"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fevdb6LduJjd","colab_type":"code","outputId":"b9ba4863-a3a3-4781-8693-ae9df97be72c","executionInfo":{"status":"error","timestamp":1555329569134,"user_tz":-330,"elapsed":43394,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":1003}},"cell_type":"code","source":["model.fit([X_train, y_train], [y_train, X_train], batch_size=50, epochs=50,\n","              validation_data=[[X_test, y_test], [y_test, X_test]], callbacks=[ m_check], shuffle= True,)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Train on 2034 samples, validate on 625 samples\n","Epoch 1/50\n"],"name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-76070e584dd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit([X_train, y_train], [y_train, X_train], batch_size=50, epochs=50,\n\u001b[0;32m----> 2\u001b[0;31m               validation_data=[[X_test, y_test], [y_test, X_test]], callbacks=[ m_check], shuffle= True,)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[50,4,54528,16] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training/Adam/gradients/digitcaps_1/MatMul_4_grad/MatMul_1}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"]}]},{"metadata":{"id":"9CorJykwPwCP","colab_type":"code","colab":{}},"cell_type":"code","source":["model.save_weights(./result + '/trained_model.h5')\n","    print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hlgr_RHVQ4cR","colab_type":"code","colab":{}},"cell_type":"code","source":["def train(model, data, args):\n","    \"\"\"\n","    Training a CapsuleNet\n","    :param model: the CapsuleNet model\n","    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n","    :param args: arguments\n","    :return: The trained model\n","    \"\"\"\n","    # unpacking the data\n","    (x_train, y_train), (x_test, y_test) = data\n","\n","    # callbacks\n","    log = callbacks.CSVLogger('./result'+ '/log.csv')\n","    tb = callbacks.TensorBoard(log_dir='./result'+ '/tensorboard-logs',\n","                               batch_size=args.batch_size, \n","    checkpoint = callbacks.ModelCheckpoint('./result' + '/weights-{epoch:02d}.h5', monitor='val_capsnet_acc',\n","                                           save_best_only=True, save_weights_only=True, verbose=1)\n","    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: 0.001 * (0.9 ** 50))\n","\n","    # compile the model\n","    model.compile(optimizer=optimizers.Adam(lr=0.001,),\n","                  loss=[margin_loss, 'mse'],\n","                  loss_weights=[1., 0.392],\n","                  metrics={'capsnet': 'accuracy'})\n","\n","    \n","    #Training without data augmentation:\n","    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n","              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n","   \n","\n","    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n","     \"\"\"\n","    def train_generator(x, y, batch_size, shift_fraction=0.):\n","        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n","                                           height_shift_range=shift_fraction)  # shift up to 2 pixel for MNIST\n","        generator = train_datagen.flow(x, y, batch_size=batch_size)\n","        while 1:\n","            x_batch, y_batch = generator.next()\n","            yield ([x_batch, y_batch], [y_batch, x_batch])\n","       \n","\n","    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n","    model.fit_generator(generator=train_generator(x_train, y_train, args.batch_size, args.shift_fraction),\n","                        steps_per_epoch=int(y_train.shape[0] / args.batch_size),\n","                        epochs=args.epochs,\n","                        validation_data=[[x_test, y_test], [y_test, x_test]],\n","                        callbacks=[log, tb, checkpoint, lr_decay])\n","    # End: Training with data augmentation -----------------------------------------------------------------------#\n","     \"\"\"\n","    model.save_weights(args.save_dir + '/trained_model.h5')\n","    print('Trained model saved to \\'%s/trained_model.h5\\'' % args.save_dir)\n","\n","    from utils import plot_log\n","    plot_log(args.save_dir + '/log.csv', show=True)\n","\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ldfyINASqJ0T","colab_type":"code","outputId":"56f3d3c0-5104-4e1d-8849-2b41e6f91634","executionInfo":{"status":"error","timestamp":1554789244018,"user_tz":-330,"elapsed":1258,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":132}},"cell_type":"code","source":[" model, eval_model, manipulate_model = CapsNet(input_shape=X_train.shape[1:],\n","                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n","                                                  routings=3)\n","    "],"execution_count":0,"outputs":[{"output_type":"error","ename":"IndentationError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-d6028096f73a>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    model.summary()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"]}]},{"metadata":{"id":"aI_ARR3kRHkQ","colab_type":"code","outputId":"c0e0ac7c-a984-4a21-fa04-3dbae46bbf81","executionInfo":{"status":"error","timestamp":1554652157252,"user_tz":-330,"elapsed":4123,"user":{"displayName":"Shah Fahad","photoUrl":"","userId":"07260500457103866977"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"cell_type":"code","source":["if __name__ == \"__main__\":\n","    import os\n","    import argparse\n","    #from keras.preprocessing.image import ImageDataGenerator\n","    from keras import callbacks\n","\n","    # setting the hyper parameters\n","    parser = argparse.ArgumentParser(description=\"Capsule Network on MNIST.\")\n","    parser.add_argument('--epochs', default=50, type=int)\n","    parser.add_argument('--batch_size', default=100, type=int)\n","    parser.add_argument('--lr', default=0.001, type=float,\n","                        help=\"Initial learning rate\")\n","    parser.add_argument('--lr_decay', default=0.9, type=float,\n","                        help=\"The value multiplied by lr at each epoch. Set a larger value for larger epochs\")\n","    parser.add_argument('--lam_recon', default=0.392, type=float,\n","                        help=\"The coefficient for the loss of decoder\")\n","    parser.add_argument('-r', '--routings', default=3, type=int,\n","                        help=\"Number of iterations used in routing algorithm. should > 0\")\n","    parser.add_argument('--shift_fraction', default=0.1, type=float,\n","                        help=\"Fraction of pixels to shift at most in each direction.\")\n","    parser.add_argument('--debug', action='store_true',\n","                        help=\"Save weights by TensorBoard\")\n","    parser.add_argument('--save_dir', default='./result')\n","    parser.add_argument('-t', '--testing', action='store_true',\n","                        help=\"Test the trained model on testing dataset\")\n","    parser.add_argument('--digit', default=5, type=int,\n","                        help=\"Digit to manipulate\")\n","    parser.add_argument('-w', '--weights', default=None,\n","                        help=\"The path of the saved weights. Should be specified when testing\")\n","    args = parser.parse_args()\n","    print(args)\n","\n","    if not os.path.exists(args.save_dir):\n","        os.makedirs(args.save_dir)\n","\n","    # load data\n","    #(x_train, y_train), (x_test, y_test) = load_mnist()\n","\n","    # define model\n","    model, eval_model, manipulate_model = CapsNet(input_shape=X_train.shape[1:],\n","                                                  n_class=len(np.unique(np.argmax(y_train, 1))),\n","                                                  routings=3)\n","    model.summary()\n","\n","    # train or test\n","    if args.weights is not None:  # init the model weights with provided one\n","        model.load_weights(args.weights)\n","    if not args.testing:\n","        train(model=model, data=((X_train, y_train), (X_test, y_test)), args=args)\n","    "],"execution_count":0,"outputs":[{"output_type":"stream","text":["usage: ipykernel_launcher.py [-h] [--epochs EPOCHS] [--batch_size BATCH_SIZE]\n","                             [--lr LR] [--lr_decay LR_DECAY]\n","                             [--lam_recon LAM_RECON] [-r ROUTINGS]\n","                             [--shift_fraction SHIFT_FRACTION] [--debug]\n","                             [--save_dir SAVE_DIR] [-t] [--digit DIGIT]\n","                             [-w WEIGHTS]\n","ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-dfee3e68-bda9-40d2-adb2-2cb2a1deabf0.json\n"],"name":"stderr"},{"output_type":"error","ename":"SystemExit","evalue":"ignored","traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"]},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n","  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"],"name":"stderr"}]},{"metadata":{"id":"JU9KZzZpQ4w6","colab_type":"code","colab":{}},"cell_type":"code","source":["def make_model():\n","    in_layer = Input(shape=(40, 200,1))\n","    x = Conv2D(32,(5,5), activation = 'elu')(in_layer)  \n","    x = Dropout(0.5)(x)\n","    x = Conv2D(64,(5,5), activation = 'elu')(x)         \n","    x = Dropout(0.5)(x)\n","    x = Conv2D(64,(5,5), activation = 'elu')(x)         \n","    x = Dropout(0.5)(x)\n","    x = Conv2D(128, (5,5))(x)                           \n","    x = GlobalAveragePooling2D()(x)                     \n","    x = Dense(64,activation='elu')(x)\n","    x = Dropout(0.5)(x)\n","    output_layer = Dense(4, activation = \"softmax\")(x) # softmax output\n","    model = Model(inputs = in_layer, outputs=output_layer)\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sVmcdWMsREWT","colab_type":"code","outputId":"550c89fa-a2c8-4050-d3b1-8f6b96b09797","executionInfo":{"status":"ok","timestamp":1554405315875,"user_tz":-330,"elapsed":5022,"user":{"displayName":"Rahul Kumar","photoUrl":"https://lh6.googleusercontent.com/-sipN3JvLZqw/AAAAAAAAAAI/AAAAAAAAARc/PRMPLKAQSf4/s64/photo.jpg","userId":"13729899591725078352"}},"colab":{"base_uri":"https://localhost:8080/","height":7361}},"cell_type":"code","source":["model  =  make_model()\n","model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\",\n","                    metrics=[\"accuracy\"])\n","m_check = keras.callbacks.ModelCheckpoint(filepath = './cnn_logfilter_default.h5', monitor='val_acc', save_best_only=True, mode='max', verbose=1 )\n","hist = model.fit(X_train, y_train, \n","                 batch_size=32, validation_data=(X_test,y_test),nb_epoch=100, verbose=1, shuffle = True,callbacks=[m_check] \n","                )"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Train on 2034 samples, validate on 625 samples\n","Epoch 1/100\n","2034/2034 [==============================] - 18s 9ms/step - loss: 2.0054 - acc: 0.3535 - val_loss: 2.9518 - val_acc: 0.2592\n","\n","Epoch 00001: val_acc improved from -inf to 0.25920, saving model to ./cnn_logfilter_default.h5\n","Epoch 2/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.4955 - acc: 0.4169 - val_loss: 1.3380 - val_acc: 0.5408\n","\n","Epoch 00002: val_acc improved from 0.25920 to 0.54080, saving model to ./cnn_logfilter_default.h5\n","Epoch 3/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.4156 - acc: 0.4326 - val_loss: 1.0730 - val_acc: 0.5504\n","\n","Epoch 00003: val_acc improved from 0.54080 to 0.55040, saving model to ./cnn_logfilter_default.h5\n","Epoch 4/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.3081 - acc: 0.4454 - val_loss: 1.2375 - val_acc: 0.5584\n","\n","Epoch 00004: val_acc improved from 0.55040 to 0.55840, saving model to ./cnn_logfilter_default.h5\n","Epoch 5/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.2425 - acc: 0.4553 - val_loss: 1.0511 - val_acc: 0.5360\n","\n","Epoch 00005: val_acc did not improve from 0.55840\n","Epoch 6/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.1653 - acc: 0.4794 - val_loss: 1.1127 - val_acc: 0.5392\n","\n","Epoch 00006: val_acc did not improve from 0.55840\n","Epoch 7/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.1685 - acc: 0.4685 - val_loss: 1.5279 - val_acc: 0.3616\n","\n","Epoch 00007: val_acc did not improve from 0.55840\n","Epoch 8/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.1703 - acc: 0.4676 - val_loss: 1.1018 - val_acc: 0.4912\n","\n","Epoch 00008: val_acc did not improve from 0.55840\n","Epoch 9/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.1322 - acc: 0.4926 - val_loss: 1.2298 - val_acc: 0.4720\n","\n","Epoch 00009: val_acc did not improve from 0.55840\n","Epoch 10/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0929 - acc: 0.5000 - val_loss: 1.0163 - val_acc: 0.5680\n","\n","Epoch 00010: val_acc improved from 0.55840 to 0.56800, saving model to ./cnn_logfilter_default.h5\n","Epoch 11/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0667 - acc: 0.5088 - val_loss: 1.4393 - val_acc: 0.3824\n","\n","Epoch 00011: val_acc did not improve from 0.56800\n","Epoch 12/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0619 - acc: 0.5177 - val_loss: 1.0335 - val_acc: 0.5792\n","\n","Epoch 00012: val_acc improved from 0.56800 to 0.57920, saving model to ./cnn_logfilter_default.h5\n","Epoch 13/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0422 - acc: 0.5393 - val_loss: 1.3791 - val_acc: 0.4112\n","\n","Epoch 00013: val_acc did not improve from 0.57920\n","Epoch 14/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0400 - acc: 0.5447 - val_loss: 1.1066 - val_acc: 0.5456\n","\n","Epoch 00014: val_acc did not improve from 0.57920\n","Epoch 15/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0304 - acc: 0.5241 - val_loss: 1.2219 - val_acc: 0.4832\n","\n","Epoch 00015: val_acc did not improve from 0.57920\n","Epoch 16/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9992 - acc: 0.5654 - val_loss: 1.0066 - val_acc: 0.6064\n","\n","Epoch 00016: val_acc improved from 0.57920 to 0.60640, saving model to ./cnn_logfilter_default.h5\n","Epoch 17/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0285 - acc: 0.5487 - val_loss: 1.0606 - val_acc: 0.5728\n","\n","Epoch 00017: val_acc did not improve from 0.60640\n","Epoch 18/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0279 - acc: 0.5418 - val_loss: 1.0889 - val_acc: 0.5136\n","\n","Epoch 00018: val_acc did not improve from 0.60640\n","Epoch 19/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0179 - acc: 0.5438 - val_loss: 1.0301 - val_acc: 0.5584\n","\n","Epoch 00019: val_acc did not improve from 0.60640\n","Epoch 20/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0228 - acc: 0.5580 - val_loss: 1.0138 - val_acc: 0.6016\n","\n","Epoch 00020: val_acc did not improve from 0.60640\n","Epoch 21/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9847 - acc: 0.5654 - val_loss: 1.0575 - val_acc: 0.5872\n","\n","Epoch 00021: val_acc did not improve from 0.60640\n","Epoch 22/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0080 - acc: 0.5536 - val_loss: 1.1344 - val_acc: 0.4832\n","\n","Epoch 00022: val_acc did not improve from 0.60640\n","Epoch 23/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0048 - acc: 0.5497 - val_loss: 1.0498 - val_acc: 0.5680\n","\n","Epoch 00023: val_acc did not improve from 0.60640\n","Epoch 24/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 1.0143 - acc: 0.5516 - val_loss: 1.3587 - val_acc: 0.4864\n","\n","Epoch 00024: val_acc did not improve from 0.60640\n","Epoch 25/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9888 - acc: 0.5639 - val_loss: 1.0445 - val_acc: 0.6032\n","\n","Epoch 00025: val_acc did not improve from 0.60640\n","Epoch 26/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9946 - acc: 0.5585 - val_loss: 1.1097 - val_acc: 0.5392\n","\n","Epoch 00026: val_acc did not improve from 0.60640\n","Epoch 27/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9717 - acc: 0.5767 - val_loss: 1.1844 - val_acc: 0.5136\n","\n","Epoch 00027: val_acc did not improve from 0.60640\n","Epoch 28/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9794 - acc: 0.5595 - val_loss: 1.0678 - val_acc: 0.5520\n","\n","Epoch 00028: val_acc did not improve from 0.60640\n","Epoch 29/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9820 - acc: 0.5678 - val_loss: 1.0100 - val_acc: 0.5872\n","\n","Epoch 00029: val_acc did not improve from 0.60640\n","Epoch 30/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9744 - acc: 0.5718 - val_loss: 0.9962 - val_acc: 0.6208\n","\n","Epoch 00030: val_acc improved from 0.60640 to 0.62080, saving model to ./cnn_logfilter_default.h5\n","Epoch 31/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9711 - acc: 0.5782 - val_loss: 1.0023 - val_acc: 0.6288\n","\n","Epoch 00031: val_acc improved from 0.62080 to 0.62880, saving model to ./cnn_logfilter_default.h5\n","Epoch 32/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9770 - acc: 0.5723 - val_loss: 1.0451 - val_acc: 0.5888\n","\n","Epoch 00032: val_acc did not improve from 0.62880\n","Epoch 33/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9439 - acc: 0.5821 - val_loss: 0.9929 - val_acc: 0.6160\n","\n","Epoch 00033: val_acc did not improve from 0.62880\n","Epoch 34/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9602 - acc: 0.5860 - val_loss: 1.0087 - val_acc: 0.6016\n","\n","Epoch 00034: val_acc did not improve from 0.62880\n","Epoch 35/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9833 - acc: 0.5615 - val_loss: 1.0132 - val_acc: 0.5808\n","\n","Epoch 00035: val_acc did not improve from 0.62880\n","Epoch 36/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9590 - acc: 0.5860 - val_loss: 1.0203 - val_acc: 0.6016\n","\n","Epoch 00036: val_acc did not improve from 0.62880\n","Epoch 37/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9512 - acc: 0.5905 - val_loss: 1.0263 - val_acc: 0.5824\n","\n","Epoch 00037: val_acc did not improve from 0.62880\n","Epoch 38/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9392 - acc: 0.6023 - val_loss: 1.0094 - val_acc: 0.5952\n","\n","Epoch 00038: val_acc did not improve from 0.62880\n","Epoch 39/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9367 - acc: 0.5855 - val_loss: 1.0630 - val_acc: 0.5744\n","\n","Epoch 00039: val_acc did not improve from 0.62880\n","Epoch 40/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9608 - acc: 0.5900 - val_loss: 1.0484 - val_acc: 0.5600\n","\n","Epoch 00040: val_acc did not improve from 0.62880\n","Epoch 41/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9201 - acc: 0.5855 - val_loss: 1.0705 - val_acc: 0.5648\n","\n","Epoch 00041: val_acc did not improve from 0.62880\n","Epoch 42/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9445 - acc: 0.5836 - val_loss: 1.0316 - val_acc: 0.5632\n","\n","Epoch 00042: val_acc did not improve from 0.62880\n","Epoch 43/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9434 - acc: 0.5934 - val_loss: 1.1599 - val_acc: 0.5248\n","\n","Epoch 00043: val_acc did not improve from 0.62880\n","Epoch 44/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9567 - acc: 0.5801 - val_loss: 1.0025 - val_acc: 0.6048\n","\n","Epoch 00044: val_acc did not improve from 0.62880\n","Epoch 45/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9189 - acc: 0.5978 - val_loss: 0.9968 - val_acc: 0.6176\n","\n","Epoch 00045: val_acc did not improve from 0.62880\n","Epoch 46/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9274 - acc: 0.5954 - val_loss: 1.0625 - val_acc: 0.5904\n","\n","Epoch 00046: val_acc did not improve from 0.62880\n","Epoch 47/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9243 - acc: 0.6082 - val_loss: 1.0133 - val_acc: 0.6192\n","\n","Epoch 00047: val_acc did not improve from 0.62880\n","Epoch 48/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9232 - acc: 0.6047 - val_loss: 1.0700 - val_acc: 0.5760\n","\n","Epoch 00048: val_acc did not improve from 0.62880\n","Epoch 49/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9325 - acc: 0.5860 - val_loss: 1.0101 - val_acc: 0.6144\n","\n","Epoch 00049: val_acc did not improve from 0.62880\n","Epoch 50/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9040 - acc: 0.6131 - val_loss: 1.0111 - val_acc: 0.6064\n","\n","Epoch 00050: val_acc did not improve from 0.62880\n","Epoch 51/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9173 - acc: 0.6111 - val_loss: 1.0139 - val_acc: 0.5952\n","\n","Epoch 00051: val_acc did not improve from 0.62880\n","Epoch 52/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9343 - acc: 0.6047 - val_loss: 1.2554 - val_acc: 0.4640\n","\n","Epoch 00052: val_acc did not improve from 0.62880\n","Epoch 53/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9021 - acc: 0.6165 - val_loss: 1.0217 - val_acc: 0.5856\n","\n","Epoch 00053: val_acc did not improve from 0.62880\n","Epoch 54/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9237 - acc: 0.5934 - val_loss: 1.0522 - val_acc: 0.5872\n","\n","Epoch 00054: val_acc did not improve from 0.62880\n","Epoch 55/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9085 - acc: 0.6077 - val_loss: 1.0357 - val_acc: 0.5792\n","\n","Epoch 00055: val_acc did not improve from 0.62880\n","Epoch 56/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9461 - acc: 0.5934 - val_loss: 1.0075 - val_acc: 0.5776\n","\n","Epoch 00056: val_acc did not improve from 0.62880\n","Epoch 57/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9368 - acc: 0.5934 - val_loss: 1.0603 - val_acc: 0.6176\n","\n","Epoch 00057: val_acc did not improve from 0.62880\n","Epoch 58/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9318 - acc: 0.5841 - val_loss: 1.0102 - val_acc: 0.6064\n","\n","Epoch 00058: val_acc did not improve from 0.62880\n","Epoch 59/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9238 - acc: 0.5998 - val_loss: 1.0210 - val_acc: 0.5936\n","\n","Epoch 00059: val_acc did not improve from 0.62880\n","Epoch 60/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9098 - acc: 0.6052 - val_loss: 0.9705 - val_acc: 0.6112\n","\n","Epoch 00060: val_acc did not improve from 0.62880\n","Epoch 61/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8862 - acc: 0.6273 - val_loss: 1.0108 - val_acc: 0.5808\n","\n","Epoch 00061: val_acc did not improve from 0.62880\n","Epoch 62/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9038 - acc: 0.6062 - val_loss: 1.0057 - val_acc: 0.6016\n","\n","Epoch 00062: val_acc did not improve from 0.62880\n","Epoch 63/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9259 - acc: 0.5983 - val_loss: 0.9736 - val_acc: 0.6096\n","\n","Epoch 00063: val_acc did not improve from 0.62880\n","Epoch 64/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8951 - acc: 0.6131 - val_loss: 1.0030 - val_acc: 0.5904\n","\n","Epoch 00064: val_acc did not improve from 0.62880\n","Epoch 65/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8854 - acc: 0.6205 - val_loss: 1.0138 - val_acc: 0.5744\n","\n","Epoch 00065: val_acc did not improve from 0.62880\n","Epoch 66/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8823 - acc: 0.6160 - val_loss: 1.0788 - val_acc: 0.5712\n","\n","Epoch 00066: val_acc did not improve from 0.62880\n","Epoch 67/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9135 - acc: 0.6121 - val_loss: 1.0288 - val_acc: 0.5728\n","\n","Epoch 00067: val_acc did not improve from 0.62880\n","Epoch 68/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8876 - acc: 0.6249 - val_loss: 0.9900 - val_acc: 0.6048\n","\n","Epoch 00068: val_acc did not improve from 0.62880\n","Epoch 69/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8981 - acc: 0.6185 - val_loss: 0.9553 - val_acc: 0.6256\n","\n","Epoch 00069: val_acc did not improve from 0.62880\n","Epoch 70/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9085 - acc: 0.6077 - val_loss: 1.0262 - val_acc: 0.6080\n","\n","Epoch 00070: val_acc did not improve from 0.62880\n","Epoch 71/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.9333 - acc: 0.6018 - val_loss: 0.9950 - val_acc: 0.6208\n","\n","Epoch 00071: val_acc did not improve from 0.62880\n","Epoch 72/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8965 - acc: 0.6155 - val_loss: 0.9774 - val_acc: 0.5920\n","\n","Epoch 00072: val_acc did not improve from 0.62880\n","Epoch 73/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8977 - acc: 0.6018 - val_loss: 1.0224 - val_acc: 0.6080\n","\n","Epoch 00073: val_acc did not improve from 0.62880\n","Epoch 74/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8940 - acc: 0.6023 - val_loss: 1.0024 - val_acc: 0.5936\n","\n","Epoch 00074: val_acc did not improve from 0.62880\n","Epoch 75/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8861 - acc: 0.6180 - val_loss: 1.2346 - val_acc: 0.4896\n","\n","Epoch 00075: val_acc did not improve from 0.62880\n","Epoch 76/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8816 - acc: 0.6146 - val_loss: 1.0841 - val_acc: 0.5744\n","\n","Epoch 00076: val_acc did not improve from 0.62880\n","Epoch 77/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8808 - acc: 0.6150 - val_loss: 0.9684 - val_acc: 0.6096\n","\n","Epoch 00077: val_acc did not improve from 0.62880\n","Epoch 78/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8819 - acc: 0.6209 - val_loss: 1.0059 - val_acc: 0.6048\n","\n","Epoch 00078: val_acc did not improve from 0.62880\n","Epoch 79/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8775 - acc: 0.6136 - val_loss: 1.1052 - val_acc: 0.5776\n","\n","Epoch 00079: val_acc did not improve from 0.62880\n","Epoch 80/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8758 - acc: 0.6278 - val_loss: 0.9516 - val_acc: 0.6176\n","\n","Epoch 00080: val_acc did not improve from 0.62880\n","Epoch 81/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8610 - acc: 0.6264 - val_loss: 1.1476 - val_acc: 0.5792\n","\n","Epoch 00081: val_acc did not improve from 0.62880\n","Epoch 82/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8935 - acc: 0.6313 - val_loss: 1.0728 - val_acc: 0.5680\n","\n","Epoch 00082: val_acc did not improve from 0.62880\n","Epoch 83/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8833 - acc: 0.6264 - val_loss: 0.9393 - val_acc: 0.6464\n","\n","Epoch 00083: val_acc improved from 0.62880 to 0.64640, saving model to ./cnn_logfilter_default.h5\n","Epoch 84/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8568 - acc: 0.6352 - val_loss: 1.0248 - val_acc: 0.5968\n","\n","Epoch 00084: val_acc did not improve from 0.64640\n","Epoch 85/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8686 - acc: 0.6205 - val_loss: 0.9234 - val_acc: 0.6352\n","\n","Epoch 00085: val_acc did not improve from 0.64640\n","Epoch 86/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8482 - acc: 0.6244 - val_loss: 1.0224 - val_acc: 0.6016\n","\n","Epoch 00086: val_acc did not improve from 0.64640\n","Epoch 87/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8715 - acc: 0.6367 - val_loss: 0.9818 - val_acc: 0.6160\n","\n","Epoch 00087: val_acc did not improve from 0.64640\n","Epoch 88/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8542 - acc: 0.6205 - val_loss: 1.0063 - val_acc: 0.6048\n","\n","Epoch 00088: val_acc did not improve from 0.64640\n","Epoch 89/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8537 - acc: 0.6337 - val_loss: 0.9893 - val_acc: 0.6384\n","\n","Epoch 00089: val_acc did not improve from 0.64640\n","Epoch 90/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8299 - acc: 0.6436 - val_loss: 1.0084 - val_acc: 0.6128\n","\n","Epoch 00090: val_acc did not improve from 0.64640\n","Epoch 91/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8300 - acc: 0.6544 - val_loss: 1.0570 - val_acc: 0.5872\n","\n","Epoch 00091: val_acc did not improve from 0.64640\n","Epoch 92/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8276 - acc: 0.6445 - val_loss: 0.9854 - val_acc: 0.6080\n","\n","Epoch 00092: val_acc did not improve from 0.64640\n","Epoch 93/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8396 - acc: 0.6391 - val_loss: 1.0340 - val_acc: 0.6144\n","\n","Epoch 00093: val_acc did not improve from 0.64640\n","Epoch 94/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8489 - acc: 0.6455 - val_loss: 1.0575 - val_acc: 0.5904\n","\n","Epoch 00094: val_acc did not improve from 0.64640\n","Epoch 95/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8342 - acc: 0.6396 - val_loss: 1.0916 - val_acc: 0.5520\n","\n","Epoch 00095: val_acc did not improve from 0.64640\n","Epoch 96/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8414 - acc: 0.6386 - val_loss: 0.9860 - val_acc: 0.5904\n","\n","Epoch 00096: val_acc did not improve from 0.64640\n","Epoch 97/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8358 - acc: 0.6298 - val_loss: 1.0096 - val_acc: 0.6000\n","\n","Epoch 00097: val_acc did not improve from 0.64640\n","Epoch 98/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8205 - acc: 0.6490 - val_loss: 0.9330 - val_acc: 0.6288\n","\n","Epoch 00098: val_acc did not improve from 0.64640\n","Epoch 99/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8293 - acc: 0.6421 - val_loss: 0.9528 - val_acc: 0.6192\n","\n","Epoch 00099: val_acc did not improve from 0.64640\n","Epoch 100/100\n","2034/2034 [==============================] - 10s 5ms/step - loss: 0.8441 - acc: 0.6411 - val_loss: 1.0322 - val_acc: 0.5968\n","\n","Epoch 00100: val_acc did not improve from 0.64640\n"],"name":"stdout"}]},{"metadata":{"id":"1El-NZJ6feqq","colab_type":"code","colab":{}},"cell_type":"code","source":["best_model = load_model('./cnn_logfilter_default.h5')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5RkBWNmPfl_8","colab_type":"code","colab":{}},"cell_type":"code","source":["y_pred = best_model.predict(X_test)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"XCUE_VIEgeAV","colab_type":"code","outputId":"0a0aaff7-5e36-4276-8f7f-2f2d82c0609e","executionInfo":{"status":"ok","timestamp":1554400843607,"user_tz":-330,"elapsed":6404,"user":{"displayName":"Rahul Kumar","photoUrl":"https://lh6.googleusercontent.com/-sipN3JvLZqw/AAAAAAAAAAI/AAAAAAAAARc/PRMPLKAQSf4/s64/photo.jpg","userId":"13729899591725078352"}},"colab":{"base_uri":"https://localhost:8080/","height":141}},"cell_type":"code","source":["y_pred"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.00766669, 0.0169646 , 0.06303743, 0.9123313 ],\n","       [0.10299007, 0.17346919, 0.47585446, 0.24768631],\n","       [0.22285958, 0.62268   , 0.1067394 , 0.04772101],\n","       ...,\n","       [0.25580174, 0.35741118, 0.37001726, 0.01676979],\n","       [0.14690156, 0.24690948, 0.42092654, 0.18526238],\n","       [0.23115315, 0.3139801 , 0.4261075 , 0.0287592 ]], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":20}]},{"metadata":{"id":"zlHwcazOflz2","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.metrics import classification_report, accuracy_score"],"execution_count":0,"outputs":[]},{"metadata":{"id":"fuHXJMR4flt2","colab_type":"code","colab":{}},"cell_type":"code","source":["for i in range(len(y_pred)):\n","  for j in range(len(y_pred[i])) :\n","    if y_pred[i][j]==max(y_pred[i]) :\n","      y_pred[i][j] = 1\n","    else:\n","      y_pred[i][j]=0"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LfMfr55Lgsig","colab_type":"code","outputId":"559a21bc-07e3-47f0-ec92-d16e79b60cf7","executionInfo":{"status":"ok","timestamp":1554405507985,"user_tz":-330,"elapsed":7597,"user":{"displayName":"Rahul Kumar","photoUrl":"https://lh6.googleusercontent.com/-sipN3JvLZqw/AAAAAAAAAAI/AAAAAAAAARc/PRMPLKAQSf4/s64/photo.jpg","userId":"13729899591725078352"}},"colab":{"base_uri":"https://localhost:8080/","height":230}},"cell_type":"code","source":["print(classification_report(y_test,y_pred))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.00      0.00      0.00        31\n","           1       0.65      0.47      0.54       174\n","           2       0.62      0.78      0.69       287\n","           3       0.70      0.74      0.72       133\n","\n","   micro avg       0.65      0.65      0.65       625\n","   macro avg       0.49      0.50      0.49       625\n","weighted avg       0.62      0.65      0.62       625\n"," samples avg       0.65      0.65      0.65       625\n","\n"],"name":"stdout"}]}]}